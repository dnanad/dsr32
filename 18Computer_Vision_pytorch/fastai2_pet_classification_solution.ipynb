{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooEVlbk9GaTb"
      },
      "source": [
        "# Image Classification with FastAI2\n",
        "\n",
        "Notebook based on\n",
        "\n",
        "* https://github.com/fastai/fastbook/blob/master/02_production.ipynb\n",
        "\n",
        "* https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb\n",
        "\n",
        "* https://github.com/fastai/fastbook/blob/master/18_CAM.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZA6YtRWF6g4"
      },
      "outputs": [],
      "source": [
        "# Make sure to go to Runtime -> Change runtime type -> GPU\n",
        "# when training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ue8ut7VDvTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ddd6f0b-ba31-4a05-a8a7-be3391be72d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 719 kB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 67.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 70.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 65.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 68.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 70.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 48.9 MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "  ## installs fastai v2 (Google Colab comes with fastai v1 by default)\n",
        "# this will also ask permision to access your Google Drive\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H4728RfofbX"
      },
      "outputs": [],
      "source": [
        "# fix for the plot_top_losses output\n",
        "#!git clone https://github.com/fastai/fastai\n",
        "#!pip install -e \"fastai[dev]\" -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuj0quSfGKpB",
        "outputId": "715bb8ae-4671-4793-8fe9-c3ce55ccd0cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/gdrive/My Drive')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# the variable gdrive now contains your Google Drive path\n",
        "# this is a pathlib Path object\n",
        "# we can create new file paths from it using the '/' operator\n",
        "# See: https://realpython.com/python-pathlib/\n",
        "from fastbook import *\n",
        "gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YQxxvOPFFhgf",
        "outputId": "632bd272-1b2b-494e-b0f1-be8d54428a2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.7.10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import fastai\n",
        "fastai.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hizx6GzHKcBl",
        "outputId": "68ac8ff5-8369-455b-dd41-41dce87e35a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image.jpeg',\n",
              " 'Programa Conferencia Internacional L&D_Unikemia & IESA.docx',\n",
              " 'Programa Conferencia Internacional L&D_Unikemia & IESA.docx.gdoc',\n",
              " 'Presupuesto.jpg',\n",
              " 'Pagos de Marzo - The Chain.gdoc']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# we can check this Path using os.listdir()\n",
        "import os\n",
        "os.listdir(gdrive)[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXcc-2ECFlDM",
        "outputId": "d2b22dd9-a3b3-4d13-e89d-cbe2e8908a2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ADULT_SAMPLE',\n",
              " 'AG_NEWS',\n",
              " 'AMAZON_REVIEWS',\n",
              " 'AMAZON_REVIEWS_POLARITY',\n",
              " 'BIWI_HEAD_POSE',\n",
              " 'BIWI_SAMPLE',\n",
              " 'CALTECH_101',\n",
              " 'CAMVID',\n",
              " 'CAMVID_TINY',\n",
              " 'CARS',\n",
              " 'CIFAR',\n",
              " 'CIFAR_100',\n",
              " 'COCO_SAMPLE',\n",
              " 'COCO_TINY',\n",
              " 'CUB_200_2011',\n",
              " 'DBPEDIA',\n",
              " 'DOGS',\n",
              " 'FLOWERS',\n",
              " 'FOOD',\n",
              " 'GOOGLE',\n",
              " 'HUMAN_NUMBERS',\n",
              " 'IMAGENETTE',\n",
              " 'IMAGENETTE_160',\n",
              " 'IMAGENETTE_320',\n",
              " 'IMAGEWANG',\n",
              " 'IMAGEWANG_160',\n",
              " 'IMAGEWANG_320',\n",
              " 'IMAGEWOOF',\n",
              " 'IMAGEWOOF_160',\n",
              " 'IMAGEWOOF_320',\n",
              " 'IMDB',\n",
              " 'IMDB_SAMPLE',\n",
              " 'LOCAL_PATH',\n",
              " 'LSUN_BEDROOMS',\n",
              " 'MACAQUES',\n",
              " 'MDL',\n",
              " 'ML_100k',\n",
              " 'ML_SAMPLE',\n",
              " 'MNIST',\n",
              " 'MNIST_SAMPLE',\n",
              " 'MNIST_TINY',\n",
              " 'MNIST_VAR_SIZE_TINY',\n",
              " 'MT_ENG_FRA',\n",
              " 'OPENAI_TRANSFORMER',\n",
              " 'PASCAL_2007',\n",
              " 'PASCAL_2012',\n",
              " 'PETS',\n",
              " 'PLANET_SAMPLE',\n",
              " 'PLANET_TINY',\n",
              " 'S3',\n",
              " 'S3_AUDI',\n",
              " 'S3_COCO',\n",
              " 'S3_IMAGE',\n",
              " 'S3_IMAGELOC',\n",
              " 'S3_MODEL',\n",
              " 'S3_NLP',\n",
              " 'SIIM_SMALL',\n",
              " 'SOGOU_NEWS',\n",
              " 'TCGA_SMALL',\n",
              " 'URL',\n",
              " 'WIKITEXT',\n",
              " 'WIKITEXT_TINY',\n",
              " 'WT103_BWD',\n",
              " 'WT103_FWD',\n",
              " 'YAHOO_ANSWERS',\n",
              " 'YELP_REVIEWS',\n",
              " 'YELP_REVIEWS_POLARITY',\n",
              " 'ZEBRA_FINCH',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " 'path']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# fastai comes with a curated list of datasets\n",
        "# See: https://docs.fast.ai/data.external.html\n",
        "dir(URLs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7xPZcf-TF3r7",
        "outputId": "ab0719af-667b-48aa-a24c-db14861ca1c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# URLs.PETS links to a dataset of dog and cat images compiled \n",
        "# by Oxford's Visual Geometry Group\n",
        "URLs.PETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7zRcH18Gn17L",
        "outputId": "2f2d8ed2-3376-4ec5-c090-1ecfdc58b044"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.7.10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import fastai\n",
        "fastai.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG83kA0poA_S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsFrgYCmfq3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "c53ae097-f4ee-4055-c7dd-0dfb43137017"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8aa9c17cad05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'pets_images_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/gdrive/My Drive/pets_images_2'"
          ]
        }
      ],
      "source": [
        "os.mkdir(gdrive / 'pets_images_2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_pWrUzPnGas"
      },
      "outputs": [],
      "source": [
        "untar_data(URLs.PETS,  data=(gdrive / 'pets_images_2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVWGEyuMg3N5"
      },
      "outputs": [],
      "source": [
        "os.listdir(Path('/content/gdrive/My Drive/pets_images_2/oxford-iiit-pet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzbF7T3koYcm"
      },
      "outputs": [],
      "source": [
        "!ls '/root/.fastai/data/oxford-iiit-pet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqqFcD9porIY"
      },
      "outputs": [],
      "source": [
        "!ls /root/.fastai/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4FfrEklJSp4"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "\n",
        "# Use gdrive and the dest argument to save the uncompressed data  \n",
        "# in a folder named 'pets_images' \n",
        "# see: https://github.com/fastai/fastai/blob/89770a495b500f585210845e195c5c9a7996f2f4/fastai/data/external.py#L244\n",
        "\n",
        "# See: https://www.geeksforgeeks.org/python-os-path-exists-method/\n",
        "if not (gdrive / 'pets_images').is_dir():\n",
        "  # https://www.geeksforgeeks.org/python-os-mkdir-method/\n",
        "  os.mkdir(gdrive / 'pets_images')\n",
        "  # this will download the data, decompress it, and return the path where the uncompressed folder is\n",
        "\n",
        "  # see: https://github.com/fastai/fastai/blob/89770a495b500f585210845e195c5c9a7996f2f4/fastai/data/external.py#L244\n",
        "  path = untar_data(URLs.PETS,  (gdrive / 'pets_images'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.PETS,  (gdrive / 'pets_images'))"
      ],
      "metadata": {
        "id": "Mf8Fr4bVo_Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"{gdrive}/pets_images\""
      ],
      "metadata": {
        "id": "_R_jeJEJoNEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9DasUsL1kE"
      },
      "outputs": [],
      "source": [
        "path = Path('/content/gdrive/My Drive/pets_images/oxford-iiit-pet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNwMGQ-dfQFV"
      },
      "outputs": [],
      "source": [
        "gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpQWbFYUfCfG"
      },
      "outputs": [],
      "source": [
        "os.listdir(gdrive / 'pets_images' / 'oxford-iiit-pet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuMlxcImNgf2"
      },
      "outputs": [],
      "source": [
        "# what we have downloaded\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z65dMprmOHQ0"
      },
      "outputs": [],
      "source": [
        "# We take a look at the images\n",
        "len(os.listdir(path / 'images')), os.listdir(path / 'images')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAd58fABjdz9"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "imag = Image.open(path / 'images' / os.listdir(path / 'images')[0])\n",
        "imag.save(Path('.')/'name.jpg')\n",
        "import os\n",
        "# this tells you your default Path (current working directory)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wOsKcWXkNNy"
      },
      "outputs": [],
      "source": [
        "os.listdir(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRJSdAlLQDnK"
      },
      "outputs": [],
      "source": [
        "# viewing a single image\n",
        "from PIL import Image\n",
        "# change the index to change the image \n",
        "index = 0\n",
        "image_filenames = os.listdir(path / 'images')\n",
        "print(image_filenames[index])\n",
        "img_pil = Image.open(path /'images'/ image_filenames[index])\n",
        "print(type(img_pil))\n",
        "import numpy as np\n",
        "print(Image.fromarray(np.array(img_pil)))\n",
        "img_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DHzhAekpcjn"
      },
      "outputs": [],
      "source": [
        "path /'images'/ image_filenames[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPmL21R4pUvv"
      },
      "outputs": [],
      "source": [
        "# turning the image into a tensor\n",
        "# https://github.com/fastai/fastai/blob/66a03da8a11cd85188f4c6f063b98c4a209492e8/fastai/vision/core.py#L91\n",
        "img_as_tensor = image2tensor(load_image(path /'images'/ image_filenames[index]))\n",
        "img_as_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCSC2uDtOLab"
      },
      "outputs": [],
      "source": [
        "# let's check the annotations file, out of curiosity \n",
        "os.listdir(path / 'annotations')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NOdnJFRPkCm"
      },
      "outputs": [],
      "source": [
        "#the space in 'My Drive' gives us some pain when train to use the path variable through bash \n",
        "!ls {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A_Z1oBiKUIn"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "Path('path_to_object')\n",
        "type(gdrive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxEY8k1WKc8d"
      },
      "outputs": [],
      "source": [
        "gdrive.stem, gdrive.root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q-R5y7bKQcg"
      },
      "outputs": [],
      "source": [
        "type(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1QUhKlDOcj_"
      },
      "outputs": [],
      "source": [
        "# we convert the path to string\n",
        "# fastai's ls() method works like os.listdir() and bash's ls  \n",
        "str(path.ls()[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YfjiegnO2-a"
      },
      "outputs": [],
      "source": [
        "# we won't need the annotations file, as the labels of the images are already in the filenames\n",
        "!cat '/content/gdrive/My Drive/pets_images/oxford-iiit-pet/annotations/test.txt' | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2L2s2kOZfT9"
      },
      "outputs": [],
      "source": [
        "image_filenames[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdQkxd09O98q"
      },
      "outputs": [],
      "source": [
        "# let's try using label extraction using regular expression\n",
        "# as shown in the notebook \n",
        "fname = (path/\"images\").ls()[0]\n",
        "# we want to discard the file extension and the number of the pet\n",
        "# we discard all characters after the _\n",
        "re.findall(r'(.+)_\\d+.jpg$', fname.name)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnona5Sve8ua"
      },
      "outputs": [],
      "source": [
        "# with str.split() we would get the same thing??\n",
        "# https://stackoverflow.com/a/48593823/45963\n",
        "str(fname.name).split('_')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f-6BqSx34Wt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "list_of_breeds = [re.findall(r'(.+)_\\d+.jpg$', fname.name) for fname in (path/\"images\").ls()]\n",
        "pd.Series([item[0] for item in list_of_breeds if isinstance(item, list) and len(item) > 0]).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE1UOyCBfWGM"
      },
      "outputs": [],
      "source": [
        "# fastai's DataBlock transforms raw data into PyTorch Datasets and Dataloaders\n",
        "# that are fed into the forward function of nn.Module subclasses\n",
        "# https://docs.fast.ai/data.block.html#DataBlock.dataloaders\n",
        "pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
        "                 # get_image_files reads the name of the image files\n",
        "                 get_items=get_image_files,\n",
        "                 # here we split the dataset into train and validation sets \n",
        "                 splitter=RandomSplitter(seed=42),\n",
        "                 #RegexLabeller uses the expression that we saw above\n",
        "                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
        "                 # these two lines implement fastai's 'presizing' data augmentation strategy\n",
        "                 # this is resizing and augmenting the data before feeding it to the trainer \n",
        "                 # it is important to use a Resize transform to make all images in a batch able to fit\n",
        "                 # into a single tensor \n",
        "                 item_tfms=Resize(460),\n",
        "                 #See: https://docs.fast.ai/vision.augment.html#aug_transforms\n",
        "                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
        "\n",
        "pets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "?aug_transforms"
      ],
      "metadata": {
        "id": "-tttVKB25w1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZozieSgi9K6"
      },
      "outputs": [],
      "source": [
        "#summary will show us how we have splitted our train and validation sets\n",
        "#https://docs.fast.ai/data.block.html#Debugging \n",
        "#https://forums.fast.ai/t/datablock-summary-is-amazing-in-v2/64632 \n",
        "pets.summary(path / 'images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ImI1xougJDJ"
      },
      "outputs": [],
      "source": [
        "# https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/25\n",
        "# we set up the seed defined for the training and validation split \n",
        "set_seed(42, True)\n",
        "dls = pets.dataloaders(path / 'images')\n",
        "dls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2Oontf9gYOz"
      },
      "outputs": [],
      "source": [
        "# We inspect a batch of the labeled data\n",
        "# We should *always* do this, as there is no guarantee\n",
        "# that the DataBlock was created correctly\n",
        "dls.show_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXDkiWqHk2KB"
      },
      "outputs": [],
      "source": [
        "dls.train.show_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FTLQRGfc3co"
      },
      "outputs": [],
      "source": [
        "# we can inspect how a single image from the training set is being augmented \n",
        "# see https://github.com/fastai/fastbook/blob/master/02_production.ipynb\n",
        "dls.train.show_batch(max_n=8, nrows=2, unique=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr14WyZyk_GM"
      },
      "outputs": [],
      "source": [
        "?dls.train.show_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clDrSp9ufd8S"
      },
      "outputs": [],
      "source": [
        "# check images in the validation set\n",
        "# https://github.com/fastai/fastbook/blob/master/02_production.ipynb\n",
        "dls.valid.show_batch(max_n=16, nrows=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiqsNU3JA11l"
      },
      "outputs": [],
      "source": [
        "# we get a batch of data as training tensor images and training tensor labels using one_batch()\n",
        "x, y = dls.one_batch()\n",
        "print(f'batch size = {len(y)}')\n",
        "# labels are encoded as integers, based on alphabetical order\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n8eqrPDmEHT"
      },
      "outputs": [],
      "source": [
        "x[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KxMcQrkm0Sp"
      },
      "outputs": [],
      "source": [
        "type(x[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYbnFkk4mV-U"
      },
      "outputs": [],
      "source": [
        "# we need to shift the order of channels\n",
        "# and then call it to be transferred to the cpu\n",
        "plt.imshow(x[0].cpu().permute(1, 2, 0));\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdRQPqKGhjcM"
      },
      "outputs": [],
      "source": [
        "# we create a convolutional netwokr trainer with cnn_learner \n",
        "# we pass it the dataloaders for traing and validation\n",
        "# specify a resnet34 as the pretrained model that we will use and define \n",
        "# error_rate (1 -accuracy)\n",
        "# try ctrl + click on cnn_learner to see its definition in Google Colab\n",
        "# Q: which is its optimization algorithm by default? \n",
        "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
        "learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biHy3s-h-zQ_"
      },
      "outputs": [],
      "source": [
        "# we can check the architecture of the PyTorch model\n",
        "# Q: What is the number of out_features in the last Linear layer? Why? \n",
        "learn.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh1_TTmZkfof"
      },
      "outputs": [],
      "source": [
        "?learn.fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU-wkx5Ywe0c"
      },
      "outputs": [],
      "source": [
        "# fine_tune trains first just the head of the model (final layers)\n",
        "# and then all layers as a second step\n",
        "# the parameters that it receives is number of epochs that it will train all layers\n",
        "# layers are using 'discriminative learning rates'\n",
        "# final layers will get higher learning rates than initial layers \n",
        "# use ctrl + click to read the the definintion of fine_tune in schedule.py\n",
        "learn.fine_tune(2, freeze_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x2y-RWipZi1"
      },
      "outputs": [],
      "source": [
        "# sample save of the model\n",
        "#learn.export(gdrive / 'pets_path'/ 'saved_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fine_tune(1)"
      ],
      "metadata": {
        "id": "1CA3OvTu_nlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEfNNwvLSBNN"
      },
      "outputs": [],
      "source": [
        "# here show a graph of the validation and training losses\n",
        "learn.recorder.plot_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHK3Ln9BGmN9"
      },
      "outputs": [],
      "source": [
        "# show results from the validation set\n",
        "learn.show_results(max_n=6, figsize=(7, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALTwhSH_Bzkn"
      },
      "outputs": [],
      "source": [
        "# We get the activations of of final layer of the neural network using learn.get_preds()\n",
        "# here we are using the same x and y data that we got from one_batch()\n",
        "pred_probs, pred_labels = learn.get_preds(dl=[(x, y)])\n",
        "# Q: Why do these have different dimensions? \n",
        "pred_probs[0].shape, pred_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ddiage0Dzs1"
      },
      "outputs": [],
      "source": [
        "# the predicted probabilities should add up to 1\n",
        "pred_probs[0].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT3-vXONGg0w"
      },
      "source": [
        "## Model interpretation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp8X9YaLH7g3"
      },
      "outputs": [],
      "source": [
        "# ClassificationIntrepretation will allow us to examine our model\n",
        "# using the validation data \n",
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8N-1OC3IAJG"
      },
      "outputs": [],
      "source": [
        "# plot a huuge confusion matrix from the validation set data\n",
        "interp.plot_confusion_matrix(figsize=(14,14), dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd2pHJbQJncB"
      },
      "outputs": [],
      "source": [
        "# check out the predictions that have been confused at least six times\n",
        "interp.most_confused(min_val=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dns3Uthl2dU"
      },
      "outputs": [],
      "source": [
        "# plot the top losses\n",
        "# prediction / actual (ground truth) label / loss value / probability (model confidence)\n",
        "#https://github.com/fastai/fastbook/blob/master/02_production.ipynb\n",
        "interp.plot_top_losses(5, nrows=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvSIaeqqms7y"
      },
      "outputs": [],
      "source": [
        "#clean mislabeled images (not very reliable, broken in the dev version)\n",
        "# won't work without importing fastai.vision.widgets\n",
        "#from fastai.vision.widgets import *\n",
        "\n",
        "#cleaner = ImageClassifierCleaner(learn)\n",
        "#cleaner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj63rtakSNI9"
      },
      "outputs": [],
      "source": [
        "# print the classification report \n",
        "# https://github.com/fastai/fastai/blob/5f3ed67f40c32df0948d6e67bfb4d08034139eb9/fastai/interpret.py#L101\n",
        "interp.print_classification_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wOvjDk_KNHy"
      },
      "source": [
        "## The Learning Rate finder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2z_2n8gPGOt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/1506.01186\n",
        "\n",
        "https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
        "\n",
        "https://blog.dataiku.com/the-learning-rate-finder-technique-how-reliable-is-it\n",
        "\n",
        "https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
        "\n",
        "https://walkwithfastai.com/lr_finder\n",
        "\n",
        "https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/\n",
        "\n",
        "https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/\n",
        "\n",
        "https://forums.fast.ai/t/interpreting-the-sched-plot-from-lr-find/12329/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHrftMP-DK9m"
      },
      "outputs": [],
      "source": [
        "?learn.fine_tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwSIpE2qKSq7"
      },
      "outputs": [],
      "source": [
        "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
        "# let's try what happens when we train using a **big** learning of 0.1\n",
        "# Q: How does this error rate compare to\n",
        "# see the definition of fine_tune, what is the base_lr that we used before? \n",
        "learn.fine_tune(1, base_lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPX-WPpr8LGw"
      },
      "source": [
        "\"Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10). Record the loss each time at each iteration and once you're finished, plot those losses against the learning rate. You'll find something like this\"\n",
        "\n",
        "\"\"The recommended minimum learning rate is the value where the loss decreases the fastest (minimum negative gradient), while the recommended maximum learning rate is 10 times less than the learning rate where the loss is minimum. Why not just the very minimum of the loss? Why 10 times less? Because what we actually plot is a smoothed version of the loss, and taking the learning rate corresponding to the minimum loss is likely to be too large and make the loss diverge during training\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmdv2zExNGpc"
      },
      "outputs": [],
      "source": [
        "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
        "# why do we care about lr_steep or lr_min\n",
        "lrs = learn.lr_find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls0NBq0kNmV0"
      },
      "outputs": [],
      "source": [
        "print(f\"Minimum/10: {lrs.valley:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learn.model"
      ],
      "metadata": {
        "id": "Jw24TyjjwA_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPwUEq3MOlCi"
      },
      "outputs": [],
      "source": [
        "\n",
        "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
        "learn.fine_tune(2, base_lr=lrs.valley)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUxb16ByEg6s"
      },
      "outputs": [],
      "source": [
        "learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79VqY1j_Z3O"
      },
      "source": [
        "## Testing with out-of-training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4HVs-NowjvR"
      },
      "outputs": [],
      "source": [
        "# let's try the model in a couple of test images of a  breed that was **not** in the training set \n",
        "import skimage.io as io\n",
        "shep1 = io.imread('https://raw.githubusercontent.com/andandandand/intro-computer-vision/main/images/German-Shepherd-on-White-00.jpg?token=AAHZIX6PZ34YFM6T6CXEN3TAHJK5W')[:,:,:3]\n",
        "print(type(shep1))\n",
        "Image.fromarray(shep1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyHSe6Y00yzj"
      },
      "outputs": [],
      "source": [
        "learn.predict(shep1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjyXBeSj-CgT"
      },
      "outputs": [],
      "source": [
        "predicted_breed,int_label,probs = learn.predict(shep1)\n",
        "print(f\"Predicted breed: {predicted_breed}.\")\n",
        "print(f\"Probability of the breed: {probs[int_label].item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp16aGPd1m0M"
      },
      "outputs": [],
      "source": [
        "shep2 = io.imread('https://raw.githubusercontent.com/andandandand/intro-computer-vision/main/images/german_shepherd.jpeg?token=AAHZIX2N2P3X35RQMDKH4JTAHJLR2')[:,:,:3]\n",
        "Image.fromarray(shep2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_fg-aI01-yC"
      },
      "outputs": [],
      "source": [
        "learn.predict(shep2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSnQ51k_-ihM"
      },
      "outputs": [],
      "source": [
        "predicted_breed,int_label,probs = learn.predict(shep2)\n",
        "print(f\"Predicted breed: {predicted_breed}.\")\n",
        "print(f\"Probability of the breed: {probs[int_label].item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDhn3qDSsr-I"
      },
      "outputs": [],
      "source": [
        "array_img_2 = io.imread('https://raw.githubusercontent.com/andandandand/images-for-colab-notebooks/main/british_shorthair.jpeg')\n",
        "from PIL import Image\n",
        "Image.fromarray(array_img_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32rk_LDms7rg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FllkEO0ErrRJ"
      },
      "outputs": [],
      "source": [
        "array_img = io.imread('https://raw.githubusercontent.com/fastai/fastbook/master/images/grizzly.jpg')\n",
        "from PIL import Image\n",
        "Image.fromarray(array_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adUDeBUisOYo"
      },
      "outputs": [],
      "source": [
        "learn.predict(array_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL77IkgM2ByM"
      },
      "outputs": [],
      "source": [
        "shep3 = io.imread('https://raw.githubusercontent.com/andandandand/intro-computer-vision/main/images/German_Shepherd_-_DSC_0346_(10096362833).jpg?token=AAHZIX5UT353RV4LNNG6XH3AHJLXO')[:,:,:3]\n",
        "Image.fromarray(shep3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ayPYpP-sLMA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylbnQWzz2t13"
      },
      "outputs": [],
      "source": [
        "learn.predict(shep3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuYvuM-G-pxX"
      },
      "outputs": [],
      "source": [
        "predicted_breed,int_label,probs = learn.predict(shep3)\n",
        "print(f\"Predicted breed: {predicted_breed}.\")\n",
        "print(f\"Probability of the breed: {probs[int_label].item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3xFZXLQ2v9n"
      },
      "outputs": [],
      "source": [
        "[imag for imag in os.listdir(path / 'images') if 'german' in imag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VGPfKg13ZO5"
      },
      "source": [
        "### CAM and GradCAM\n",
        "\n",
        "https://github.com/fastai/fastbook/blob/master/18_CAM.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ58X6PG3QBI"
      },
      "outputs": [],
      "source": [
        "path = Path('/content/gdrive/My Drive/pets_images/oxford-iiit-pet')\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJxuKxoCTk0G"
      },
      "outputs": [],
      "source": [
        "# let's explore the behavior of the classifier \n",
        "# in the dataset all cat images have a filename that start with uppercase\n",
        "# dog images start with lowercase \n",
        "def is_dog(x): return x[0].islower()\n",
        "\n",
        "dls = ImageDataLoaders.from_name_func(\n",
        "    path, get_image_files(path), valid_pct=0.2, seed=21,\n",
        "    label_func=is_dog, item_tfms=Resize(224))\n",
        "\n",
        "# we call this binary classifier dog_learn, to avoid clashes with the one \n",
        "# for pet breeds \n",
        "dog_learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
        "dog_learn.fine_tune(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKU4jMsrUnfb"
      },
      "outputs": [],
      "source": [
        "# call PILImage.create on shep1\n",
        "img = PILImage.create(shep1)\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlab7bYLrjsQ"
      },
      "outputs": [],
      "source": [
        "# we put the image in a batch of the testing (not validation) set\n",
        "x, = first(dls.test_dl([img]))\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psa3g5WysiB_"
      },
      "outputs": [],
      "source": [
        "# \"For CAM we want to store the activations of the last convolutional layer. \n",
        "#  We put our hook function in a class so it has a state that we can access later, and just store a copy of the output:\"\n",
        "class Hook():\n",
        "    def hook_func(self, m, i, o): self.stored = o.detach().clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1AbJt0Ps4Qd"
      },
      "outputs": [],
      "source": [
        "# We can then instantiate a Hook and attach it to the layer we want, which is the last layer of the CNN body:\n",
        "hook_output = Hook()\n",
        "hook_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_1sfzy6xDOL"
      },
      "outputs": [],
      "source": [
        "# the model is at model[0]\n",
        "dog_learn.model[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9ZRdpNOw5ms"
      },
      "outputs": [],
      "source": [
        "hook = dog_learn.model[0].register_forward_hook(hook_output.hook_func)\n",
        "hook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXHqNUTdxBSd"
      },
      "outputs": [],
      "source": [
        "# feed the image through the model\n",
        "with torch.no_grad(): output = dog_learn.model.eval()(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPWQCJqKyrRl"
      },
      "outputs": [],
      "source": [
        "# access our stored activations\n",
        "act = hook_output.stored[0]\n",
        "act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm-v4JuTy06A"
      },
      "outputs": [],
      "source": [
        "# double check predictions\n",
        "F.softmax(output, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2MIbdFP0H0z"
      },
      "outputs": [],
      "source": [
        "# our model is confident that the picture is a dog \n",
        "dls.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRwBVrEF3wdu"
      },
      "outputs": [],
      "source": [
        "# this is the final activation layer\n",
        "dog_learn.model[1][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I4rT3D638fD"
      },
      "outputs": [],
      "source": [
        "# and here are its weights\n",
        "dog_learn.model[1][-1].weight.shape, dog_learn.model[1][-1].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNervyhS03iX"
      },
      "outputs": [],
      "source": [
        "# matrix multiplication using einstein summation, between the weights and the activation \n",
        "# https://pytorch.org/docs/stable/generated/torch.einsum.html\n",
        "# check the dimensions of both parts\n",
        "print(dog_learn.model[1][-1].weight.shape, act.shape)\n",
        "# using 'ck,kij->cij' which values are c,k,i, and j? \n",
        "cam_map = torch.einsum('ck,kij->cij', dog_learn.model[1][-1].weight, act)\n",
        "cam_map.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH-3YHo3BW3r"
      },
      "outputs": [],
      "source": [
        "cam_map[1].detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGhjvZuuBjgQ"
      },
      "outputs": [],
      "source": [
        "np.array(cam_map[1].detach().cpu()).min(), np.array(cam_map[1].detach().cpu()).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-AqMVlR07Fw"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# Why do we use decode() and dls.train? \n",
        "x_dec = TensorImage(dls.train.decode((x,))[0][0])\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "# added colorbar \n",
        "# https://matplotlib.org/3.1.1/gallery/axes_grid1/simple_colorbar.html#sphx-glr-gallery-axes-grid1-simple-colorbar-py\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRsjPZUl1JQx"
      },
      "outputs": [],
      "source": [
        "# remove the hook to prevent memory leakage\n",
        "# hook.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPqXe_TP2Q9i"
      },
      "outputs": [],
      "source": [
        "# Same thing, now using a context manager\n",
        "# we won't go deeper into this \n",
        "class Hook():\n",
        "    def __init__(self, m):\n",
        "        self.hook = m.register_forward_hook(self.hook_func)   \n",
        "    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n",
        "    def __enter__(self, *args): return self\n",
        "    def __exit__(self, *args): self.hook.remove()\n",
        "\n",
        "\n",
        "with Hook(dog_learn.model[0]) as hook:\n",
        "    with torch.no_grad(): output = dog_learn.model.eval()(x.cuda())\n",
        "    act = hook.stored\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEoEprf32pWo"
      },
      "source": [
        "## Gradient CAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdvzAq_-8oh2"
      },
      "source": [
        "## Rationale: \n",
        "\n",
        "\n",
        "\n",
        "> The gradients of the output of the last layer with respect to the input of that layer are equal to the layer weights, since it is a linear layer.\n",
        "\n",
        "> With deeper layers, we still want the gradients, but they won't just be equal to the weights anymore. We have to calculate them. The gradients of every layer are calculated for us by PyTorch during the backward pass, but they're not stored (except for tensors where requires_grad is True). We can, however, register a hook on the backward pass, which PyTorch will give the gradients to as a parameter, so we can store them there. For this we will use a HookBwd class that works like Hook, but intercepts and stores gradients instead of activations:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpafCl1o9dlc"
      },
      "outputs": [],
      "source": [
        "class HookBwd():\n",
        "    def __init__(self, m):\n",
        "        self.hook = m.register_backward_hook(self.hook_func)   \n",
        "    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n",
        "    def __enter__(self, *args): return self\n",
        "    def __exit__(self, *args): self.hook.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xx8w8Rp9gdl"
      },
      "outputs": [],
      "source": [
        "cls = 1\n",
        "with HookBwd(dog_learn.model[0]) as hookg:\n",
        "    with Hook(dog_learn.model[0]) as hook:\n",
        "        output = dog_learn.model.eval()(x.cuda())\n",
        "        act = hook.stored\n",
        "    output[0,cls].backward()\n",
        "    grad = hookg.stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a_jAc_d9g2a"
      },
      "outputs": [],
      "source": [
        "w = grad[0].mean(dim=[1,2], keepdim=True)\n",
        "cam_map = (w * act[0]).sum(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0XFlZ_P9oCq"
      },
      "outputs": [],
      "source": [
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "# added colorbar \n",
        "# https://matplotlib.org/3.1.1/gallery/axes_grid1/simple_colorbar.html#sphx-glr-gallery-axes-grid1-simple-colorbar-py\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAAoA3Qa--D3"
      },
      "outputs": [],
      "source": [
        "# this represents the second-to-last Resnet group\n",
        "dog_learn.model[0][-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12Hq3YhU_jmu"
      },
      "outputs": [],
      "source": [
        "def get_cam_map(index):\n",
        "  with HookBwd(dog_learn.model[0][index]) as hookg:\n",
        "    with Hook(dog_learn.model[0][index]) as hook:\n",
        "        output = dog_learn.model.eval()(x.cuda())\n",
        "        act = hook.stored\n",
        "    output[0,cls].backward()\n",
        "    grad = hookg.stored\n",
        "\n",
        "  w = grad[0].mean(dim=[1,2], keepdim=True)\n",
        "  cam_map = (w * act[0]).sum(0)\n",
        "  return cam_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWz7iaw7-qQa"
      },
      "outputs": [],
      "source": [
        "cam_map = get_cam_map(-2)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeCxsM_Y-1wM"
      },
      "outputs": [],
      "source": [
        "cam_map = get_cam_map(-3)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5N53uKJAX2Z"
      },
      "outputs": [],
      "source": [
        "cam_map = get_cam_map(-4)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbBaOwWPAbRt"
      },
      "outputs": [],
      "source": [
        "cam_map = get_cam_map(-5)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFtZN_7aAfwU"
      },
      "outputs": [],
      "source": [
        "cam_map = get_cam_map(-6)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHFJYXC9AktN"
      },
      "outputs": [],
      "source": [
        "# Would get_cam_map(-7) work? \n",
        "# check dog_learn.model[0][-7]\n",
        "cam_map = get_cam_map(-8)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9NtimfcAqIF"
      },
      "outputs": [],
      "source": [
        "cam_map = get_cam_map(-1)\n",
        "_,ax = plt.subplots()\n",
        "x_dec.show(ctx=ax)\n",
        "im = ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0),\n",
        "              interpolation='bilinear', cmap='magma');\n",
        "\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "\n",
        "plt.colorbar(im, cax=cax);"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}