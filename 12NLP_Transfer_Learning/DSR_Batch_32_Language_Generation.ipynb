{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-JegRfUe4OPQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import models, layers\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Where the text files are going to live.\n",
        "dataset_path = \"dataset\"\n",
        "dataset_path_all = os.path.join(dataset_path, \"all\")\n",
        "dataset_path_train = os.path.join(dataset_path, \"train\")\n",
        "dataset_path_valid = os.path.join(dataset_path, \"valid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBsuje5a4Vir",
        "outputId": "3ed7bdae-a517-4831-ca25-ea0eb07b2ef0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Just use 20 files.\n",
        "# file_number = 20\n",
        "\n",
        "# # Gather the corpus if it has not been gathered yet.\n",
        "# if not os.path.exists(dataset_path):\n",
        "\n",
        "#     # Create all the folders.\n",
        "#     for path in [dataset_path, dataset_path_all, dataset_path_train, dataset_path_valid]:\n",
        "#         if not os.path.exists(path):\n",
        "#             os.mkdir(path)\n",
        "\n",
        "#     # Clone the repo.\n",
        "#     !git clone https://github.com/vilmibm/lovecraftcorpus\n",
        "        \n",
        "#     # Find all the files.\n",
        "#     paths_all = glob.glob(\"lovecraftcorpus/*.txt\")\n",
        "#     print(sorted(paths_all))\n",
        "\n",
        "#     # Do not use all.\n",
        "#     paths_all = paths_all[:file_number]\n",
        "\n",
        "#     # Split 80/20.\n",
        "#     split_index = int(len(paths_all) * 0.8)\n",
        "#     paths_train = paths_all[:split_index]\n",
        "#     paths_valid = paths_all[split_index:]\n",
        "\n",
        "#     # Copy files.\n",
        "#     def copy(paths, destination):\n",
        "#         for path in paths:\n",
        "#             shutil.copy2(path, destination)\n",
        "#     copy(paths_all, dataset_path_all)\n",
        "#     copy(paths_train, dataset_path_train)\n",
        "#     copy(paths_valid, dataset_path_valid)\n",
        "\n",
        "#     # Delete repo.\n",
        "#     !del -rf lovecraftcorpus\n",
        "\n",
        "#     # Done.\n",
        "#     print(\"Corpus downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNPibGnr4Y7d",
        "outputId": "420a68af-1546-42f3-e7be-90d10018a2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all\n",
            "train\n",
            "valid\n",
            "\n",
            "alchemist.txt\n",
            "arthur_jermyn.txt\n",
            "azathoth.txt\n",
            "beast.txt\n",
            "beyond_wall_of_sleep.txt\n",
            "book.txt\n",
            "celephais.txt\n",
            "charles_dexter_ward.txt\n",
            "clergyman.txt\n",
            "colour_out_of_space.txt\n",
            "cool_air.txt\n",
            "crawling_chaos.txt\n",
            "cthulhu.txt\n",
            "dagon.txt\n",
            "descendent.txt\n",
            "doorstep.txt\n",
            "dreams_in_the_witch.txt\n",
            "dunwich.txt\n",
            "erich_zann.txt\n",
            "ex_oblivione.txt\n",
            "\n",
            "alchemist.txt\n",
            "arthur_jermyn.txt\n",
            "azathoth.txt\n",
            "beast.txt\n",
            "beyond_wall_of_sleep.txt\n",
            "book.txt\n",
            "celephais.txt\n",
            "charles_dexter_ward.txt\n",
            "clergyman.txt\n",
            "colour_out_of_space.txt\n",
            "cool_air.txt\n",
            "crawling_chaos.txt\n",
            "cthulhu.txt\n",
            "dagon.txt\n",
            "descendent.txt\n",
            "doorstep.txt\n",
            "\n",
            "dreams_in_the_witch.txt\n",
            "dunwich.txt\n",
            "erich_zann.txt\n",
            "ex_oblivione.txt\n"
          ]
        }
      ],
      "source": [
        "!dir /b dataset\n",
        "print(\"\")\n",
        "!dir /b dataset\\all\n",
        "print(\"\")\n",
        "!dir /b dataset\\train\n",
        "print(\"\")\n",
        "!dir /b dataset\\valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kDRW9cNT4aRZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20 files belonging to 1 classes.\n",
            "Found 16 files belonging to 1 classes.\n",
            "Found 4 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "def create_dataset(dataset_path):\n",
        "    dataset = preprocessing.text_dataset_from_directory(\n",
        "        dataset_path,\n",
        "        labels=None,\n",
        "        batch_size=batch_size,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset_original_all = create_dataset(dataset_path_all)\n",
        "dataset_original_train = create_dataset(dataset_path_train)\n",
        "dataset_original_valid = create_dataset(dataset_path_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for x in dataset_original_all.take(1):\n",
        "#     print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'of', 'to', 'a', 'in', 'was', 'he', 'had', 'that', 'his', 'i', 'it', 'as', 'with', 'at', 'which', 'from', 'for', 'on', 'but', 'not', 'were', 'by', 'him', 'be', 'all', 'an', 'this', 'they', 'there', 'or', 'no', 'my', 'when', 'could', 'one', 'have', 'been', 'would', 'what', 'some', 'so', 'old', 'about', 'me', 'its', 'out', 'more', 'is', 'only', 'seemed', 'their', 'now', 'very', 'before', 'who', 'up', 'did', 'you', 'after', 'then', 'time', 'into', 'them', 'than', 'ward', 'must', 'found', 'any', 'even', 'where', 'man', 'like', 'down', 'if', 'came', 'through', 'strange', 'her', 'great', 'though', 'never', 'upon', 'those', 'saw', 'house', 'night', 'willett', 'heard', 'thing', 'room', 'over', 'other', 'whose', 'she', 'curwen', 'come']\n"
          ]
        }
      ],
      "source": [
        "vacabulary_size = 10_000\n",
        "\n",
        "\n",
        "encoder = layers.TextVectorization(\n",
        "    max_tokens=vacabulary_size,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "\n",
        "encoder.adapt(dataset_original_all)\n",
        "\n",
        "\n",
        "vocabulary = encoder.get_vocabulary()\n",
        "\n",
        "print(vocabulary[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   2 5452  274 ...    0    0    0]\n",
            " [   2  334    4 ...  848 5359  581]\n",
            " [   2 5194  590 ...    0    0    0]\n",
            " ...\n",
            " [ 124    2  271 ...    0    0    0]\n",
            " [7391   13  266 ...    0    0    0]\n",
            " [1923  249   61 ...    0    0    0]]\n",
            "[[   2  417  160 ...   67    9   60]\n",
            " [ 225    7    2 ...    0    0    0]\n",
            " [   2 1011    4 ...    0    0    0]\n",
            " [   1    1   36 ...    0    0    0]]\n"
          ]
        }
      ],
      "source": [
        "sequence_length = 32\n",
        "\n",
        "def create_dataset_for_autoregression(dataset):\n",
        "      x_inputs = []\n",
        "      y_inputs = []\n",
        "\n",
        "      for books in dataset:\n",
        "            books = encoder(books).numpy()\n",
        "            print(books)\n",
        "            break # TODO Remove me alter\n",
        "      \n",
        "      return tf.data.Dataset.from_tensor_slices((x_inputs, y_inputs))\n",
        "\n",
        "dataset_train = create_dataset_for_autoregression(dataset_original_train)\n",
        "dataset_valid = create_dataset_for_autoregression(dataset_original_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tf-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b35987021c60fd5d1e03bdac54f8fbaabbcd0a19fb1d4d763cb68ffaa03d4a8d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
