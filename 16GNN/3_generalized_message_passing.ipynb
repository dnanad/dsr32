{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Please, make a copy of the notebook before we start.\n",
        "# Turn on the GPU support in the Runtime/Change runtime type.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# Install Pytorch Geometric and its dependencies\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "DFApxvwPg9-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f74cd5-1cf4-4dac-976e-f93f8a554a8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 6.4 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **[Generalized message passing](https://arxiv.org/abs/1806.01261) and relational inductive bias**\n",
        "\n",
        ">\"Infinite use of finite means\" (Humboldt,1836; Chomsky, 1965)\n",
        "\n",
        "*   Human intellect is able to productively compose complex structures (sentences) using a small set of elements (words).\n",
        "*   *Relational inductive bias* imposes constraints on relationships and interactions among entities in a learning process. It's a prior knowledge one incorporates into a learning algorithm. For example, for a CNN it's locality and **translation invariance**.\n",
        "*   A graph structure imposes a strong relational inductive bias, since during the learning process we heavily utilize the relations between the nodes, which can be of arbitrary nature.\n",
        "*   Message passing algorithms must provide permutation invariance of the nodes, since it doesn't matter from what neighbour we receive the signal first. Therefore, graph neural networks have **permutation invariance**.\n",
        "  - **Example:** compute the centre of mass of the solar system $\\to$ the order of the planets doesn't matter."
      ],
      "metadata": {
        "id": "r4s_0uj9EtRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Graph definition:**\n",
        "\n",
        "*   $\\mathcal{G}=(\\mathbf{u}, V,E)$\n",
        "  - $V$: nodes.\n",
        "  - $E$: edges.\n",
        "  - $\\mathbf{u}$: global attribute.\n",
        "*   The graph is:\n",
        "  - Directed.\n",
        "  - Attributed (nodes, edges, global).\n",
        "  - Multi-graph: there can be more than one edge between nodes.\n",
        "\n",
        "**Example:** balls connected by springs in the gravitational field:\n",
        "*   $\\mathbf{u}$ is the total kinetic energy of the system.\n",
        "*   $V= \\{v_k \\}$ is the set of balls, with attributes for position and momentum.\n",
        "*   $E = \\{(\\textbf{e}_k, r_k, s_k)\\}$ is the set of springs connecting the balls, with their corresponding potential energies, $r_k$ and $s_k$ are the indices of the receiver and sender nodes.\n",
        "\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"> <img src =\"https://drive.google.com/uc?id=17jaILJ0oiN9OzwZ6ZowI1hLHowcPBl_3\" width=600 height=275></center>"
      ],
      "metadata": {
        "id": "0QaAIZZqPLUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GN block update**\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\t\t\\textbf{e}^{'}_{k} &= \\phi^e (\\textbf{e}_k, \\, \\textbf{v}_{r_k}, \\, \\textbf{v}_{r_s}, \\, \\textbf{u}) \\\\ \n",
        "    \\bar{\\textbf{e}}'_{i} &= \\rho^{e \\to v} ( E^{'}_i) \\\\\n",
        "    \\textbf{v}^{'}_{i} &= \\phi^{v} (\\bar{\\textbf{e}}'_{i}, \\, \\textbf{v}_{i}, \\, \\textbf{u}) \\hspace{1.3cm} \\\\\n",
        "    \\bar{\\textbf{e}}' &= \\rho^{e \\to u} ( E^{'}) \\\\\n",
        "    \\bar{\\textbf{v}}' &=  \\rho^{v \\to u} (V^{'}) \\\\\n",
        "    \\textbf{u}' &= \\phi^{u} (\\bar{\\textbf{e}}', \\, \\bar{\\textbf{v}}', \\, \\textbf{u}) \\hspace{1.3cm} \\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "*   $E^{'}_i = \\{(\\textbf{e}^{'}_k, r_k, s_k)\\}_{r_k = i, k=1:N^{|E|}}$ is the set of all per-edge outputs of the node $i$.\n",
        "*   $E^{'} = \\bigcup E^{'}_i $ is the set of all per-edge outputs.\n",
        "*   $V^{'} = \\{v^{'}_i \\}_{i=1:N^{|V|}}$ is the set of all per-node outputs.\n",
        "\n",
        "Ball system example:\n",
        "\n",
        "1. Update the corresponding forces (edge attributes) between two connected balls.\n",
        "2. Aggregate the edge updates for edges that project to vertex $i$. Sum all the forces acting on a ball $i$.\n",
        "3. Update node attributes, e.g. update a position and a momentum of each ball.\n",
        "4. Aggregate all updated edge attributes. Sum all the forces.\n",
        "5. Aggregate all nodes attributes, which might correspond to calculating the total momentum of the system.\n",
        "6. Update the global attribute, e.g. the kinetic energy of the system.\n",
        "\n",
        "**All different functions $\\phi$ can be arbitrary differentiable functions, e.g. neural networks. And all functions $\\rho$ are aggregation functions, e.g. *sum* or *mean*.** (figure credit - [Peter Battaglia](https://arxiv.org/abs/1806.01261))\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"> <img src =\"https://drive.google.com/uc?id=12I2j2TQtWHWSBZBJ3wYtkZHN45sux62t\" width=700 height=175></center>"
      ],
      "metadata": {
        "id": "ZP0dIieBSAWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graph classification with generalized message passing**"
      ],
      "metadata": {
        "id": "QVZAkGIMaGTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Classify an entire graph instead of single nodes or edges with a given dataset of multiple graphs.\n",
        "*   Molecular property prediction, in which molecules are represented as graphs. Each atom is linked to a node, and edges in the graph are the bonds between atoms. \n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"> <img src =\"https://drive.google.com/uc?id=1o_BXW38qr-9uGGPPGgfghCz7P1OOKpsh\" width=600></center>\n",
        "\n",
        "We will use a slightly modified version of the MUTAG dataset. Let's add a global attribute to each graph and set it to zero.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LRQ4yuxZd3OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "def add_global_attr(data):\n",
        "  data.u = torch.tensor([[0]]).to(torch.float32)\n",
        "  return data\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG', transform=add_global_attr)\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[20]  # Get the first graph.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "id": "JsvRSAKcMPia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076097e7-1efb-4940-bd7a-46c697376bbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 38], x=[18, 7], edge_attr=[38, 4], y=[1], u=[1, 1])\n",
            "=============================================================\n",
            "Number of nodes: 18\n",
            "Number of edges: 38\n",
            "Average node degree: 2.11\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Node features are the one-hot encoded atom types.\n",
        "*   Edge features are different types of atom bonds.\n",
        "*   The binary graph labels represent a graph's \"mutagenic effect on a specific gram negative bacterium\". (Not so important for us)\n",
        "*   We added a global attribute to each graph.\n",
        "\n",
        "Let's shuffle the dataset and use the first 150 graphs as training graphs, while using the remaining ones for testing:"
      ],
      "metadata": {
        "id": "DFIkCvEcolLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "xPBLVsjlMRtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19cb9896-e5df-46ee-cb17-3b3d05fa7c4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mini-batching of graphs in PyG**\n",
        "\n",
        "*   Each graph in the batch can have a different number of nodes and edges, hence we would require a lot of padding to obtain a single tensor. \n",
        "*   Represent $N$ graphs in a batch as a single large graph with concatenated node and edge lists.\n",
        "* There is no edge between different graphs.\n",
        "\n",
        "<center width=\"100%\" style=\"padding:10px\"> <img src =\"https://drive.google.com/uc?id=1C5Ob2YQxrMH-Xf55mZ2RNO79q2Rw5E9P\" width=600></center>\n",
        "\n",
        "Advantages over other batching procedures:\n",
        "\n",
        "1. GNN operators that rely on a message passing scheme do not need to be modified since messages are not exchanged between two nodes that belong to different graphs.\n",
        "\n",
        "2. There is no computational or memory overhead since adjacency matrices are saved in a sparse fashion holding only non-zero entries, *i.e.*, the edges.\n",
        "\n",
        "**PyG's Dataloader does batching for us automatically.**"
      ],
      "metadata": {
        "id": "sjmSM1sVqBRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "metadata": {
        "id": "V6gJjanVMTOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4630102-8fda-4595-d8b1-37b25e884243"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2632], x=[1184, 7], edge_attr=[2632, 4], y=[64], u=[64, 1], batch=[1184], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2508], x=[1147, 7], edge_attr=[2508, 4], y=[64], u=[64, 1], batch=[1147], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "DataBatch(edge_index=[2, 854], x=[383, 7], edge_attr=[854, 4], y=[22], u=[22, 1], batch=[383], ptr=[23])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each `Batch` object is equipped with a **`batch` vector**, which maps each node to its respective graph in the batch:\n",
        "\n",
        "$$\n",
        "\\textrm{batch} = [ 0, \\ldots, 0, 1, \\ldots, 1, 2, \\ldots ]\n",
        "$$"
      ],
      "metadata": {
        "id": "Biut_wdBsZXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Graph level predictions**\n",
        "\n",
        "Graph classification in a nutshell:\n",
        "\n",
        "1. Embed each node by performing multiple rounds of message passing.\n",
        "2. Aggregate node embeddings, edge embeddings and the global embedding into a unified graph embedding (**readout layer**).\n",
        "3. Train a final classifier on the graph embedding.\n",
        "\n",
        "The most common **readout layers** is to simply take the average of node embeddings:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{z}^{(L)}_v\n",
        "$$\n",
        "\n",
        "We can do it via [`torch_geometric.nn.global_mean_pool`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_mean_pool)."
      ],
      "metadata": {
        "id": "Bu2Al-1utajI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train a GN model**\n",
        "\n",
        "Let's create three neural networks for node, edge and global attributes updates. You can check [torch_scatter.scatter_mean](https://pytorch-scatter.readthedocs.io/en/1.3.0/functions/mean.html) function for better understanding.\n"
      ],
      "metadata": {
        "id": "eNuF2bsnx06p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_scatter import scatter_mean\n",
        "\n",
        "# TODO: complete the edge, node and global update networks\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    num_features = dataset.num_features * 2 + dataset.num_edge_features\n",
        "    num_features += dataset[0].u.size(1)\n",
        "    self.edge_mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(num_features, hidden_channels),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Dropout(0.5),\n",
        "        torch.nn.Linear(hidden_channels, dataset.num_edge_features),\n",
        "        torch.nn.ReLU()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, src, dest, edge_attr, u, batch):\n",
        "    # src, dest: [E, F_x], where E is the number of edges.\n",
        "    # edge_attr: [E, F_e]\n",
        "    # u: [B, F_u], where B is the number of graphs.\n",
        "    # batch: [E] with max entry B - 1.\n",
        "    out = torch.cat([src, dest, edge_attr, u[batch]], 1)  \n",
        "    return self.edge_mlp(out) \n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    num_features = dataset.num_features + dataset.num_edge_features\n",
        "    num_features += dataset[0].u.size(1)\n",
        "    self.node_mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(num_features, hidden_channels),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Dropout(0.5),\n",
        "        torch.nn.Linear(hidden_channels, dataset.num_features),\n",
        "        torch.nn.ReLU()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "    # x: [N, F_x], where N is the number of nodes in all graphs of the batch.\n",
        "    # edge_index: [2, E] with max entry N - 1.\n",
        "    # edge_attr: [E, F_e]\n",
        "    # u: [B, F_u]\n",
        "    # batch: [N] with max entry B - 1.\n",
        "    dest_node_idx = edge_index[1]\n",
        "    # Average all attr of incoming edges for each dest node\n",
        "    edge_out_bar = scatter_mean(src=edge_attr, index=dest_node_idx, dim=0)\n",
        "    out = torch.cat([x, edge_out_bar, u[batch]], 1)\n",
        "    return self.node_mlp(out), edge_out_bar\n",
        "    \n",
        "\n",
        "class GlobalModel(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    num_features = dataset.num_features + dataset.num_edge_features\n",
        "    num_features += dataset[0].u.size(1)\n",
        "    self.global_mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(num_features, hidden_channels),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Dropout(0.5),\n",
        "        torch.nn.Linear(hidden_channels, 1),\n",
        "        torch.nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, node_attr_prime, edge_out_bar, u, batch):\n",
        "    # node_attr_bar: [N, F_x], where N is the number of nodes in the batch.\n",
        "    # edge_attr: [N, F_e]\n",
        "    # u: [B, F_u]\n",
        "    # batch: [N] with max entry B - 1.\n",
        "    # Average all node attributes for each graph, using batch tensor. \n",
        "    node_attr_bar = scatter_mean(node_attr_prime, batch, 0)\n",
        "    # Average all edge attributes for each graph, using batch\n",
        "    edge_attr_bar = scatter_mean(edge_out_bar, batch, 0)\n",
        "    out = torch.cat([u, node_attr_bar, edge_attr_bar], dim=1)\n",
        "    return self.global_mlp(out)"
      ],
      "metadata": {
        "id": "gqtdyriJfw1E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then let's write the GN class that takes 3 update models as its' arguments. `num_passes` is the number of times we perform node, edge, global updates."
      ],
      "metadata": {
        "id": "kwQDNkbt6dFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GN(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, edge_model, node_model, global_model, num_passes):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    self.edge_model = edge_model\n",
        "    self.node_model = node_model\n",
        "    self.global_model = global_model\n",
        "    num_features = dataset.num_features + dataset.num_edge_features\n",
        "    num_features += dataset[0].u.size(1)\n",
        "    self.lin = torch.nn.Linear(num_features, dataset.num_classes)\n",
        "    self.num_passes = num_passes\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for item in [self.node_model, self.edge_model, self.global_model]:\n",
        "      if hasattr(item, 'reset_parameters'):\n",
        "        item.reset_parameters()\n",
        "\n",
        "  # TODO: write the forward pass together\n",
        "  def forward(self, node_attr, edge_attr, u, edge_index, batch):\n",
        "    #1. perform message passing to obtain graph embeddings\n",
        "    for _ in range(self.num_passes):\n",
        "      src_node_idx = edge_index[0]\n",
        "      dest_node_idx = edge_index[1]\n",
        "\n",
        "      edge_attr = self.edge_model(\n",
        "          node_attr[src_node_idx], node_attr[dest_node_idx], edge_attr,\n",
        "          u, batch[src_node_idx] # to convert batch to the dim E\n",
        "      )\n",
        "\n",
        "      node_attr, edge_out_bar = self.node_model(\n",
        "          node_attr, edge_index, edge_attr, u, batch\n",
        "      )\n",
        "\n",
        "      u = self.global_model(node_attr, edge_out_bar, u, batch)\n",
        "\n",
        "    #2. Readout later\n",
        "    graph_attr = torch.cat([node_attr, edge_out_bar, u[batch]], dim=1)\n",
        "    graph_attr = global_mean_pool(graph_attr, batch)\n",
        "\n",
        "    return self.lin(graph_attr)\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "      return (f'{self.__class__.__name__}(\\n'\n",
        "              f'  edge_model={self.edge_model},\\n'\n",
        "              f'  node_model={self.node_model},\\n'\n",
        "              f'  global_model={self.global_model}\\n'\n",
        "              f')')"
      ],
      "metadata": {
        "id": "GY22vszgaEvP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can start training:"
      ],
      "metadata": {
        "id": "nQvY5EWFPAoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "edge_model, node_model, global_model = EdgeModel(64), NodeModel(64), GlobalModel(64)\n",
        "NUM_PASSES = 3\n",
        "\n",
        "gn_model = GN(edge_model, node_model, global_model, NUM_PASSES)\n",
        "optimizer = torch.optim.Adam(gn_model.parameters(), lr=0.01)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(gn_model)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "def train():\n",
        "    gn_model.train()\n",
        "\n",
        "    for data in train_loader: # Iterate over the batches of grahs\n",
        "         out = gn_model(data.x, data.edge_attr, data.u, data.edge_index, data.batch)  # Forward pass(es)\n",
        "         loss = loss_function(out, data.y)  # Compute the loss\n",
        "         loss.backward() # Compute the gradients\n",
        "         optimizer.step()  # Update the weights based on the computed gradients\n",
        "         optimizer.zero_grad() # Clear the computed gradients\n",
        "\n",
        "def test(loader):\n",
        "     gn_model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  \n",
        "         out = gn_model(data.x, data.edge_attr, data.u, data.edge_index, data.batch)  # Iterate over the batches \n",
        "         pred = out.argmax(dim=1)  # Predict the labels using the label with the highest probability\n",
        "         correct += int((pred == data.y).sum())  # Check against the ground truth\n",
        "     return correct / len(loader.dataset) # Compute accuracy\n",
        "\n",
        "for epoch in range(1, 120):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "IorF01wyMXra",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "137ac22a-e63e-4cad-ed21-e0322d5c4ee4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GN(\n",
            "  edge_model=EdgeModel(\n",
            "  (edge_mlp): Sequential(\n",
            "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (4): ReLU()\n",
            "  )\n",
            "),\n",
            "  node_model=NodeModel(\n",
            "  (node_mlp): Sequential(\n",
            "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=7, bias=True)\n",
            "    (4): ReLU()\n",
            "  )\n",
            "),\n",
            "  global_model=GlobalModel(\n",
            "  (global_mlp): Sequential(\n",
            "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (4): ReLU()\n",
            "  )\n",
            ")\n",
            ")\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 012, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 013, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 014, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 015, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 016, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 017, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 018, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 019, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 020, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 021, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 022, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 023, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 024, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 025, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 026, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 027, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 028, Train Acc: 0.6933, Test Acc: 0.7105\n",
            "Epoch: 029, Train Acc: 0.7000, Test Acc: 0.7632\n",
            "Epoch: 030, Train Acc: 0.6933, Test Acc: 0.7105\n",
            "Epoch: 031, Train Acc: 0.6933, Test Acc: 0.7105\n",
            "Epoch: 032, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 034, Train Acc: 0.7000, Test Acc: 0.7105\n",
            "Epoch: 035, Train Acc: 0.7267, Test Acc: 0.7105\n",
            "Epoch: 036, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 037, Train Acc: 0.7467, Test Acc: 0.7368\n",
            "Epoch: 038, Train Acc: 0.7400, Test Acc: 0.7368\n",
            "Epoch: 039, Train Acc: 0.7400, Test Acc: 0.7368\n",
            "Epoch: 040, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 041, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 042, Train Acc: 0.7467, Test Acc: 0.7368\n",
            "Epoch: 043, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 044, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 045, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 046, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 047, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 048, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 049, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 050, Train Acc: 0.7600, Test Acc: 0.7368\n",
            "Epoch: 051, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 052, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 053, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 054, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 056, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 057, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 058, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 059, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 060, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 061, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 062, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 063, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 064, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 065, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 066, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 067, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 068, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 069, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 070, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 071, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 074, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 075, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 076, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 077, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 078, Train Acc: 0.8400, Test Acc: 0.7895\n",
            "Epoch: 079, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 081, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 082, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 083, Train Acc: 0.8133, Test Acc: 0.7895\n",
            "Epoch: 084, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 085, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 086, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 087, Train Acc: 0.8133, Test Acc: 0.8158\n",
            "Epoch: 088, Train Acc: 0.8333, Test Acc: 0.7632\n",
            "Epoch: 089, Train Acc: 0.8333, Test Acc: 0.7632\n",
            "Epoch: 090, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 091, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 092, Train Acc: 0.8467, Test Acc: 0.7368\n",
            "Epoch: 093, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 094, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 095, Train Acc: 0.8333, Test Acc: 0.7632\n",
            "Epoch: 096, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 097, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 099, Train Acc: 0.8400, Test Acc: 0.7368\n",
            "Epoch: 100, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 101, Train Acc: 0.8400, Test Acc: 0.7632\n",
            "Epoch: 102, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 103, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 104, Train Acc: 0.8400, Test Acc: 0.7368\n",
            "Epoch: 105, Train Acc: 0.8200, Test Acc: 0.7368\n",
            "Epoch: 106, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 107, Train Acc: 0.8467, Test Acc: 0.7368\n",
            "Epoch: 108, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 109, Train Acc: 0.8133, Test Acc: 0.7895\n",
            "Epoch: 110, Train Acc: 0.8400, Test Acc: 0.7632\n",
            "Epoch: 111, Train Acc: 0.8467, Test Acc: 0.7368\n",
            "Epoch: 112, Train Acc: 0.8467, Test Acc: 0.7368\n",
            "Epoch: 113, Train Acc: 0.8467, Test Acc: 0.7368\n",
            "Epoch: 114, Train Acc: 0.8333, Test Acc: 0.7368\n",
            "Epoch: 115, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 116, Train Acc: 0.8200, Test Acc: 0.7632\n",
            "Epoch: 117, Train Acc: 0.8333, Test Acc: 0.7632\n",
            "Epoch: 118, Train Acc: 0.8333, Test Acc: 0.7105\n",
            "Epoch: 119, Train Acc: 0.8467, Test Acc: 0.7105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reasons for the fluctations in accuracy can be explained by the rather small dataset (only 38 test graphs), and usually disappear once one applies GNNs to larger datasets."
      ],
      "metadata": {
        "id": "F2w3bA9S7oaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3.1: Play around with the GN block**\n",
        "\n",
        "Since, it's our last exercise for today, let's make it relaxed.\n",
        "\n",
        "*   Check how the accuracy changes when you increase the number of passes. \n",
        "  - **hint:** set to 1, 3, 5, 10. \n",
        "  - What do you observe? Do you have an idea why?\n",
        "*   Feel free to play around with the architectures of the update networks:\n",
        "  - You can add more layers, more neurons, change dropout, etc. See how the result changes.\n",
        "  - You can swap `scatter_mean` with `scatter_sum`, `scatter_max`, `scatter_min`. Do you see any change in the result?"
      ],
      "metadata": {
        "id": "bmhMGL16Ai7V"
      }
    }
  ]
}